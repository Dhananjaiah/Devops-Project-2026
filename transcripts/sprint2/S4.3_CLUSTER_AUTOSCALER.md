# S4.3: Cluster Autoscaler Setup

**Video Duration:** ~8 minutes  
**Story Points:** 2

---

## Learning Objectives

By the end of this video, you will be able to:
- Understand Cluster Autoscaler vs Karpenter
- Configure IRSA for autoscaler
- Install Cluster Autoscaler via Helm
- Test autoscaling behavior

---

## Transcript

### Introduction (0:00 - 1:00)

Welcome back! Our cluster has 2 nodes, but what happens when we deploy 50 pods that need more resources? We need autoscaling!

Cluster Autoscaler automatically adds nodes when pods can't be scheduled and removes nodes when they're underutilized.

### Cluster Autoscaler vs Karpenter (1:00 - 2:00)

Quick comparison:

**Cluster Autoscaler** - The standard. Works with ASGs. Slower but battle-tested.

**Karpenter** - AWS's newer solution. Faster, more flexible, but more complex.

For this course, we'll use Cluster Autoscaler. It's simpler and you'll see it in most production environments.

### Setting Up IRSA (2:00 - 4:00)

The autoscaler needs AWS permissions to modify ASGs. We use IRSA - IAM Roles for Service Accounts.

```hcl
module "cluster_autoscaler_irsa" {
  source  = "terraform-aws-modules/iam/aws//modules/iam-role-for-service-accounts-eks"
  
  role_name = "${var.project_name}-${var.environment}-cluster-autoscaler"
  
  attach_cluster_autoscaler_policy = true
  cluster_autoscaler_cluster_names = [aws_eks_cluster.main.name]
  
  oidc_providers = {
    main = {
      provider_arn   = aws_iam_openid_connect_provider.eks.arn
      namespace      = "kube-system"
      service_account = "cluster-autoscaler"
    }
  }
}
```

This creates an IAM role that only the cluster-autoscaler service account can assume.

### Installing via Helm (4:00 - 6:00)

```bash
helm repo add autoscaler https://kubernetes.github.io/autoscaler
helm repo update

helm install cluster-autoscaler autoscaler/cluster-autoscaler \
  -n kube-system \
  --set autoDiscovery.clusterName=techitfactory-dev \
  --set awsRegion=ap-south-1 \
  --set rbac.serviceAccount.annotations."eks\.amazonaws\.com/role-arn"=$AUTOSCALER_ROLE_ARN
```

The key settings:
- `autoDiscovery.clusterName` - finds node groups automatically
- `eks.amazonaws.com/role-arn` annotation - enables IRSA

Let's verify:

```bash
kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-cluster-autoscaler
kubectl logs -n kube-system -l app.kubernetes.io/name=aws-cluster-autoscaler
```

### Testing Autoscaling (6:00 - 7:30)

Let's trigger a scale-up:

```bash
kubectl create deployment inflate --image=public.ecr.aws/eks-distro/kubernetes/pause:3.7
kubectl scale deployment inflate --replicas=50
```

*[Waits a moment]*

```bash
kubectl get pods
kubectl get nodes
```

You'll see pods in Pending state, then after a minute or two, new nodes appear!

*[Shows nodes scaling up]*

Now scale down:

```bash
kubectl delete deployment inflate
```

After about 10 minutes of low utilization, nodes will be removed.

### Summary (7:30 - 8:00)

We set up:
1. IRSA role for secure AWS access
2. Cluster Autoscaler via Helm
3. Auto-discovery of node groups
4. Tested scale-up behavior

Your cluster now grows and shrinks automatically!

---

## Key Takeaways

- Cluster Autoscaler adjusts node count based on demand
- IRSA provides secure AWS credentials
- Auto-discovery finds node groups by tags
- Scale-up takes 1-2 minutes
- Scale-down has a cooldown period (~10 min)
