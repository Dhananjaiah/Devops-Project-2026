Epic 1: Course Foundation: Repos, Standards, and Trunk-Based Workflow
StoryAcceptance Criteria (DoD)Sub-tasks (Tasks)Create 3 production repos (Infra / App / GitOps) with baseline docs- Infra repo exists with /bootstrap and /modules skeleton- App monorepo exists with /services/* and baseline standards- GitOps repo exists with App-of-Apps structure- Each repo has README: prerequisites + local run + deploy flow- Naming + branching model documented (Trunk-based)- Create infra repo skeleton (bootstrap, modules, CI folders)- Create app monorepo skeleton (/services/* + repo docs)- Create GitOps repo skeleton (apps/platform/app-of-apps layout)- Add course-grade READMEs + diagrams placeholdersEnforce Trunk-Based Development rules using GitHub branch protections- main protected: PR required, no direct push, no force push- Required checks enabled (lint/test/scan as applicable)- PR template + issue templates exist- CODEOWNERS set for infra/app/gitops areas- Tag/release conventions documented for dev→prod promotion- Configure branch protection rules + required status checks- Add PR template + CODEOWNERS + contribution guidelines- Document release/tag promotion flow for learners

Epic 2: Terraform Bootstrap: Remote State + IAM (SSO-ready) + GitHub OIDC
StoryAcceptance Criteria (DoD)Sub-tasks (Tasks)Build Terraform bootstrap stack (S3 state bucket + DynamoDB lock + KMS)- S3 state bucket created with versioning + encryption enabled- DynamoDB lock table created and used for state locking- KMS key created (and used for encryption where applicable)- Other stacks can terraform init with remote backend successfully- Bootstrap is idempotent (re-run safe)- Implement bootstrap Terraform (S3 + DynamoDB + KMS)- Add backend configuration examples + environment variables doc- Validate idempotency: apply twice without drift- Add destroy safety notes (what to keep vs destroy)Configure GitHub Actions → AWS auth using OIDC (no static AWS keys)- AWS IAM OIDC provider created for GitHub- Least-privilege IAM role created for infra pipeline- Workflow can run terraform init/plan successfully using OIDC- apply runs only on protected branch (main) and is auditable- Course notes explain OIDC and security benefits- Create IAM OIDC provider + role trust policy for GitHub- Create IAM policies for Terraform apply (scoped)- Add GitHub workflow to assume role and run terraform plan/applyCreate Terraform production module skeleton and repo conventions- Module structure exists: modules/vpc, modules/eks, modules/platform-bootstrap- Root stacks follow naming, tagging, and variables conventions- CI runs terraform fmt + validate + tflint (or equivalent)- Outputs defined for cluster and gateway/DNS targets- Course runbook added (apply/destroy flow)- Create modules skeleton + root stacks wiring- Add CI: fmt/validate/lint for Terraform- Define standard tags + outputs used by GitOps

Epic 3: Networking (Cost-Optimized): VPC + Subnets + Single NAT + Endpoints
StoryAcceptance Criteria (DoD)Sub-tasks (Tasks)Provision VPC with multi-AZ public/private subnets and single NAT- VPC created in ap-south-1 with 2 AZs and public/private subnets- Single NAT gateway used (cost-optimized) with correct routing- Security group baseline documented (least privilege)- Standard tags applied (owner, env, cost, project)- Destroy removes all networking resources cleanly- Implement VPC module (subnets, routes, IGW, NAT)- Add outputs needed by EKS module (subnet ids, vpc id)- Write verification steps (route tables, NAT egress)Add VPC endpoints to reduce NAT costs and improve boot speed- S3 gateway endpoint enabled for private subnets- Interface endpoints added where appropriate (document list)- Endpoints managed by Terraform and removed on destroy- Course note explains cost/time benefits- Basic measurement captured (before/after)- Add S3 gateway endpoint- Add interface endpoints list (as per baseline)- Document measurable impact + tradeoffs

Epic 4: EKS Cluster Baseline: Managed Node Groups + Autoscaling + Required Add-ons
StoryAcceptance Criteria (DoD)Sub-tasks (Tasks)Provision EKS cluster + managed node group(s) using Terraform modules- EKS cluster created via Terraform with version pinning- Managed node group created (min/desired/max)- Cluster logging configured per baseline and documented- kubectl access works with SSO-mapped admin role- Cluster OIDC issuer output is available (for IRSA)- Implement EKS module wiring (cluster + nodegroup)- Enable cluster logging + add outputs- Validate cluster health (nodes ready, system pods running)Configure AWS SSO (IAM Identity Center) access mapping for EKS admins- SSO role mapped to Kubernetes RBAC (admin)- Document steps for students to configure kubectl access- Access tested from clean workstation flow- No long-lived AWS keys required for human access- Troubleshooting section included (auth failures, kubeconfig)- Configure EKS access entries / aws-auth mapping for SSO role- Write student guide: aws sso login → kubeconfig → kubectl get nodes- Add troubleshooting for common SSO/EKS auth issuesInstall Cluster Autoscaler with IRSA and validate scale-out- Cluster Autoscaler deployed successfully- IRSA configured (no node IAM creds dependence)- Scale-out test proves node scale-out- Scale-in verified after load reduces- Runbook documents how to demo autoscaling in class- Create IRSA role/policy for autoscaler- Deploy Cluster Autoscaler and configure nodegroup discovery- Perform scale-out/scale-in demo and capture screenshots/logsInstall baseline EKS add-ons required in Sprint 1 (Terraform-managed)- metrics-server installed and working- EBS CSI driver installed (baseline)- All add-ons are version-pinned and reproducible- Health verification checklist exists- Add-ons are removed cleanly on destroy- Install metrics-server- Install EBS CSI driver (with IRSA if applicable)- Create add-ons verification checklist (kubectl + logs)

Epic 5: Gateway API + Domain: Edge Gateway on EKS + Route53-ready for techitfactory.com
StoryAcceptance Criteria (DoD)Sub-tasks (Tasks)Install Envoy Gateway (Gateway API controller) with AWS-ready service exposure- Controller installed successfully on EKS- GatewayClass is available and accepted- Gateway routes HTTP traffic to backend services- Course standard for routing is documented (Gateway + HTTPRoute)- Destroy removes gateway-related resources cleanly- Install Envoy Gateway (Helm, version pinned)- Apply GatewayClass/Gateway/HTTPRoute and validate routing- Document gateway patterns used in the course (listeners, PathPrefix routing)Configure Route53 + TLS baseline for dev entrypoint (dev.techitfactory.com)- DNS record created for dev.techitfactory.com pointing to the gateway edge- TLS enabled end-to-end using chosen easy approach- Renewal/validation flow documented- Smoke test URL returns 200- Teardown plan documented for build/destroy- Provision TLS cert and validation (recommended: ACM)- Create Route53 record(s) for dev entrypoint- Write smoke test script (curl) + troubleshooting

Epic 6: ArgoCD Production Bootstrap + App-of-Apps (Separate GitOps Repo)
StoryAcceptance Criteria (DoD)Sub-tasks (Tasks)Install ArgoCD using official manifests (version pinned) with secure course access- ArgoCD installed in argocd namespace via official manifests (pinned version)- UI reachable (port-forward for demo)- Admin access handled securely (no secrets in Git)- Install is reproducible and documented- Course note covers components + basic hardening- Install ArgoCD via official manifests (version pinned)- Expose ArgoCD UI for course demo (port-forward)- Document access steps (admin secret, SSO later)Bootstrap App-of-Apps pointing to GitOps repo root application (Kustomize-first)- Root Argo Application created (platform-root)- Child apps sync from GitOps repo automatically- GitOps repo uses Kustomize overlays per env (dev/prod)- Dev uses auto-sync policy; prod is controlled- Repo structure scalable (apps/platform/services/envs)- Demo: empty cluster → running apps via ArgoCD- Create GitOps repo layout + root app manifest- Add Kustomize base/overlays (dev/prod) and document conventions- Configure Argo sync policy for dev/prod separation- Demo script + screenshots for courseCreate namespaces + Argo Projects: dev and prod (blast-radius control)- dev and prod namespaces exist- Argo Projects restrict destinations and sources- Dev apps deploy only to dev by default- Promotion model documented (release/tag gate)- Basic RBAC practices documented- Create namespaces dev/prod- Create Argo Projects with restrictions- Document why Projects prevent accidental prod deploys

Epic 7: Observability (PLG): Prometheus + Loki + Grafana
StoryAcceptance Criteria (DoD)Sub-tasks (Tasks)Deploy kube-prometheus-stack and expose Grafana for the course demo- Prometheus scrapes cluster metrics successfully- Grafana reachable (port-forward for demo; gateway exposure optional)- At least one dashboard shows node/pod metrics- Config managed via GitOps- Troubleshooting: targets down/scrape errors documented- Install kube-prometheus-stack via GitOps- Expose Grafana UI for course demo (port-forward) and document access- Add baseline dashboards and verify dataDeploy Loki and integrate logs in Grafana- Loki deployed and receiving logs from workloads- Grafana Loki datasource configured- Logs visible for at least one microservice and ArgoCD- Debug workflow documented (metrics → logs → pod)- Config managed via GitOps and reproducible- Install Loki stack + log collector (baseline)- Configure Grafana datasource and verify queries- Write log troubleshooting guide (labels, retention)

Epic 8: Walking Skeleton App: Polyglot Services + Dockerfiles + Helm Charts
StoryAcceptance Criteria (DoD)Sub-tasks (Tasks)Create monorepo service structure with standardized service conventions- Repo has /services/frontend, /services/product, /services/cart, /services/order- Each service exposes /health and /ready- Logs to stdout in consistent format- Config via env vars documented (PORT, LOG_LEVEL)- Each service has local run instructions- Create service folder structure and baseline READMEs- Implement health endpoints (all services)- Standardize config and logging conventionsCreate production-grade Dockerfiles for all services and verify builds- Docker build succeeds for each service- Containers run as non-root where feasible- docker run + curl /health returns 200- Tagging scheme is consistent (service + git sha)- Images ready to push to DockerHub- Create Dockerfiles (multi-stage where applicable)- Add .dockerignore and security baseline (non-root)- Local build & run verification for all servicesCreate Helm charts for all services and deploy to dev namespace- Each service has Helm chart with Deployment + Service + probes- Requests/limits set (baseline)- Dev values exist; Prod values placeholders exist- ArgoCD deploy works into dev namespace- Rollback explained (Git revert)- Scaffold Helm charts for all services- Add probes/resources and environment values- Create Argo app-of-apps entries for servicesExpose a single Dev entrypoint and validate end-to-end routing- dev.techitfactory.com serves frontend- API routes reach backend health endpoints (product/cart/order)- Smoke test script exists (curl checks)- Grafana shows metrics; Loki shows logs for request- Demo checklist created for course recording- Create Gateway API routes (host/path) for frontend and APIs- Write smoke test script and add to repo- Verify observability for a sample request (metrics + logs)

Epic 9: CI/CD: Per-service GitHub Actions + Dev Auto-Deploy + Prod Promotion by Release
StoryAcceptance Criteria (DoD)Sub-tasks (Tasks)Implement GitHub Actions per service: build/test/Trivy scan/push to DockerHub- Each service has its own workflow under .github/workflows- On PR: lint + unit tests run- On merge: docker build + Trivy scan + push to DockerHub- DockerHub credentials stored as GitHub secrets- Logs are course-friendly and easy to follow- Create workflows for all services (independent)- Add Trivy image scan step + fail policy- Push to DockerHub with consistent taggingIntegrate SonarCloud for code quality gates (per service)- SonarCloud connected to GitHub and runs on PR- Quality gate status visible in PR- Policy documented: block merge or warn- Course example shows failing rule and fix- Tokens stored in GitHub secrets- Create SonarCloud projects + configure tokens- Add sonar analysis step to workflows- Document quality gate policy + demo scenarioDev auto-deploy on merge by updating GitOps repo with new image tags- Merge to main updates GitOps dev values with new image tag- ArgoCD auto-sync deploys to dev namespace- Traceability documented (commit → image tag → running pod)- Rollback documented (revert GitOps commit)- No manual kubectl apply for app deployments- Implement workflow step to update GitOps repo (dev)- Configure Argo auto-sync for dev apps- Write traceability and rollback notesProd promotion via GitHub Release/Tag (manual gate) updates GitOps prod- GitHub Release/Tag triggers promotion workflow- Workflow updates GitOps prod values to released version- Argo deploys to prod namespace only from release workflow- Release notes template exists for course- Post-deploy smoke test runs (or documented)- Create release-triggered workflow for promotion- Update prod values in GitOps repo- Add smoke test step + publish release instructions

Epic 10: Daily Build & Destroy Automation: One-command Up/Down + <20 min target tracking
StoryAcceptance Criteria (DoD)Sub-tasks (Tasks)Create one-command platform up/down scripts (make up / make down)- make up provisions VPC → EKS → add-ons → ArgoCD → GitOps sync- make down destroys all created resources cleanly- Scripts are safe to re-run (idempotent)- Verification steps ensure no orphaned AWS resources- Recovery steps documented for common failures- Create make up orchestration (terraform apply + bootstrap)- Create make down orchestration (terraform destroy)- Write cleanup verification checklist (LB/NLB, NAT, EIP, ENI)Capture boot-time metrics and enforce time budget tracking (<20 min target)- Terraform apply steps are timed and logged- End-of-sprint demo includes real boot-time measurement- Optimization checklist documented (top 5 speed levers)- Known bottlenecks captured with Sprint 2 plan- Students can reproduce timing method- Add timing to scripts (timestamps per stage)- Document performance levers (node size, add-ons, endpoints)- Create sprint demo checklist with measured boot time
