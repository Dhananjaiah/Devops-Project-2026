# S4.1: EKS Cluster Provisioning

**Video Duration:** ~15 minutes  
**Story Points:** 5

---

## Learning Objectives

By the end of this video, you will be able to:
- Create an EKS cluster with Terraform
- Configure managed node groups
- Understand control plane vs data plane
- Enable essential EKS add-ons

---

## Transcript

### Introduction (0:00 - 1:00)

Welcome back! This is the big one - we're creating our EKS cluster. This is where all our microservices will run.

EKS is Amazon's managed Kubernetes service. Amazon handles the control plane - API server, etcd, schedulers - while we manage the worker nodes.

### EKS Architecture Overview (1:00 - 3:00)

*[Shows architecture diagram]*

The control plane is fully managed by AWS:
- High availability across 3 AZs
- Automatic upgrades and patching
- Managed etcd with backups

We just need to create:
- The cluster itself
- Node groups (EC2 instances)
- IAM roles for access
- Security groups

Let me show you the code.

### Creating the EKS Cluster (3:00 - 6:00)

```hcl
resource "aws_eks_cluster" "main" {
  name     = "${var.project_name}-${var.environment}"
  role_arn = aws_iam_role.cluster.arn
  version  = var.cluster_version

  vpc_config {
    subnet_ids              = var.subnet_ids
    endpoint_private_access = true
    endpoint_public_access  = true
    security_group_ids      = [aws_security_group.cluster.id]
  }

  enabled_cluster_log_types = [
    "api",
    "audit",
    "authenticator",
    "controllerManager",
    "scheduler"
  ]

  depends_on = [
    aws_iam_role_policy_attachment.cluster_policy
  ]
}
```

Let me break this down:

`endpoint_private_access = true` - Worker nodes access API server privately.
`endpoint_public_access = true` - We can access from our laptops.
`enabled_cluster_log_types` - All control plane logs go to CloudWatch.

### Cluster IAM Role (6:00 - 7:00)

```hcl
resource "aws_iam_role" "cluster" {
  name = "${var.project_name}-${var.environment}-eks-cluster"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{
      Action = "sts:AssumeRole"
      Effect = "Allow"
      Principal = {
        Service = "eks.amazonaws.com"
      }
    }]
  })
}

resource "aws_iam_role_policy_attachment" "cluster_policy" {
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKSClusterPolicy"
  role       = aws_iam_role.cluster.name
}
```

Standard cluster role - EKS needs this to manage resources.

### Managed Node Group (7:00 - 10:00)

```hcl
resource "aws_eks_node_group" "main" {
  cluster_name    = aws_eks_cluster.main.name
  node_group_name = "${var.project_name}-${var.environment}-nodes"
  node_role_arn   = aws_iam_role.node.arn
  subnet_ids      = var.private_subnet_ids

  scaling_config {
    desired_size = 2
    min_size     = 2
    max_size     = 5
  }

  instance_types = ["t3.medium"]

  update_config {
    max_unavailable = 1
  }

  labels = {
    role = "general"
  }
}
```

Key decisions:
- `t3.medium` - 2 vCPU, 4GB RAM. Good for learning, cheap.
- `min_size = 2` - High availability across AZs
- `max_size = 5` - Room for autoscaling
- Nodes in private subnets - security best practice

### Node IAM Role (10:00 - 11:00)

```hcl
resource "aws_iam_role" "node" {
  name = "${var.project_name}-${var.environment}-eks-node"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{
      Action = "sts:AssumeRole"
      Effect = "Allow"
      Principal = {
        Service = "ec2.amazonaws.com"
      }
    }]
  })
}

resource "aws_iam_role_policy_attachment" "node_policies" {
  for_each = toset([
    "arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy",
    "arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy",
    "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly",
  ])

  policy_arn = each.value
  role       = aws_iam_role.node.name
}
```

Nodes need these policies for networking (CNI), ECR access, and EKS operations.

### Deploying (11:00 - 13:00)

```bash
cd techitfactory-infra/environments/dev
terraform plan
terraform apply
```

*[Shows apply - takes 10-15 minutes]*

EKS cluster creation takes a while - about 10-15 minutes. AWS is provisioning the control plane across three AZs.

*[Fast forward]*

Done! Let's connect to it.

### Connecting to the Cluster (13:00 - 14:30)

```bash
aws eks update-kubeconfig --name techitfactory-dev --region ap-south-1

kubectl get nodes
```

*[Shows two nodes]*

We have two worker nodes, both in Ready state!

```bash
kubectl get ns
kubectl get pods -A
```

You can see the system pods running - CoreDNS, kube-proxy, CNI pods.

### Summary (14:30 - 15:00)

We created:
1. EKS cluster with private/public endpoints
2. Managed node group with 2 t3.medium instances
3. Proper IAM roles for cluster and nodes
4. Control plane logging enabled

Next: Configuring EKS access with IAM!

---

## Key Takeaways

- EKS control plane is fully managed by AWS
- Managed node groups handle node lifecycle
- Nodes should be in private subnets
- Control plane logging is essential for debugging
- Cluster creation takes 10-15 minutes
